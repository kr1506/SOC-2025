
#SOC-2025
# My Learning Summary from Recent Tutorials

This document summarizes the key concepts I learned from the following two tutorials:

1. [Cosyne 2022 Tutorial on Spiking Neural Networks - Part 1/2](https://www.youtube.com/watch?v=GTXTQ_sOxak)
2. [Reinforcement Learning in 3 Hours | Full Course using Python](https://www.youtube.com/watch?v=G-GwYp7D1gc) 

Iâ€™ve included what I already knew before watching these videos and the new insights and skills I gained.

## 1. Cosyne 2022 Tutorial on Spiking Neural Networks (Part 1/2)

### âœ… What I Knew Before

- Basic understanding of artificial neurons in traditional neural networks
- Familiarity with artificial neural networks (ANNs) using activations and weighted sums
- Awareness that biological neurons communicate through electrical spikes
- General concept that machine learning models are inspired by the brain

### ðŸ†• What I Learned

- What **Spiking Neural Networks (SNNs)** are and how they differ fundamentally from ANNs
- Detailed neuron models:
    - Integrate-and-fire model
    - Hodgkin-Huxley model
- Concepts of information representation through:
    - Temporal coding (information encoded in spike timing)
    - Population coding (information encoded across groups of neurons)
- Biological learning mechanisms like **Spike-Timing-Dependent Plasticity (STDP)**
- Applications of SNNs in:
    - Neuroscience modeling
    - Energy-efficient computing
    - Machine learning tasks
- Understanding the challenges of training SNNs, especially compared to backpropagation in ANNs
- Insights into neuromorphic hardware designed to run SNNs efficiently

---

## 2. Reinforcement Learning in 3 Hours | Full Course using Python

### âœ… What I Knew Before

- Basic principles of **Reinforcement Learning (RL):**
    - Agents learn by interacting with an environment
    - Concepts of rewards, states, and actions
- High-level awareness of Q-learning
- Knowledge that RL is used in applications like gaming, robotics, and autonomous systems
- Familiarity with Python libraries such as NumPy and PyTorch

### ðŸ†• What I Learned

- Mathematical foundation of **Markov Decision Processes (MDPs)** and their importance in RL
- Bellman equations and how they relate to value functions
- Differences among key RL methods:
    - Dynamic Programming
    - Monte Carlo methods
    - Temporal Difference (TD) learning
- Detailed workings of **Q-learning**:
    - How it updates value estimates
    - How it approximates optimal policies
- Introduction to **Deep Q-Networks (DQN)**:
    - How neural networks estimate Q-values
    - Stabilization techniques like target networks and experience replay
- Policy Gradient methods and how they differ from value-based methods
- Practical coding in Python for RL:
    - Setting up environments using OpenAI Gym
    - Implementing learning algorithms step by step
- Important challenges in RL:
    - Exploration vs. exploitation balance
    - Sample inefficiency
    - High variance during training

---

## ðŸ’¡ Overall Takeaways

- Developed a much clearer understanding of how biologically inspired neural networks, like SNNs, operate and why theyâ€™re important for both neuroscience and efficient computing
- Gained practical and theoretical knowledge of RL:
    - Understood the differences between various RL algorithms
    - Learned how to implement RL techniques in Python
- Appreciated the intersection between neuroscience and machine learning
- Identified new areas to explore further, including:
    - Neuromorphic hardware for SNNs
    - Advanced RL algorithms for real-world applications

---

## Next Steps

- Experiment with coding simple Spiking Neural Network simulations
- Implement small RL projects using OpenAI Gym and Python
- Explore research papers referenced in both tutorials for deeper insights

